{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, FloatType, StringType, LongType, StructField\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/19 09:48:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kyles-mbp.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Book Ratings</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x167a1e8d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[6]\").appName('Book Ratings')\\\n",
    "                            .config('spark.executor.memory', '8g')\\\n",
    "                            .config('spark.driver.memory', '4g')\\\n",
    "                            .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset into Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get underlying Spark Context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new schema\n",
    "schema = StructType()\\\n",
    "        .add('bookID', IntegerType(), nullable=False)\\\n",
    "        .add('userID', StringType(), nullable=False)\\\n",
    "        .add('rating', FloatType(), nullable=False)\\\n",
    "        .add('timestamp', LongType(), nullable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(bookID=1713353, userID='A1C6M8LCIX4M6M', rating=5.0, timestamp=1123804800)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data into PySpark DF\n",
    "books = spark.read.format('csv').schema(schema).load('../data/Books.csv')\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bookID: integer (nullable = true)\n",
      " |-- userID: string (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify data types\n",
    "books.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is returned in the format `(int(bookID), str(userID), float(rating), long(timestamp))`.\n",
    "\n",
    "To parse into a `PySpark` `Rating` object, it is expected to be in the format `(int(userID), int(bookID), float(rating), long(timestamp))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for `NoneType` Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DF has at least 6671993 rows with a None value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for None values in the DF\n",
    "none_count = books.filter(\n",
    "    (col(\"bookID\").isNull()) |\n",
    "    (col(\"userID\").isNull()) |\n",
    "    (col(\"rating\").isNull()) |\n",
    "    (col(\"timestamp\").isNull())\n",
    ").count()\n",
    "has_none_values = none_count > 0\n",
    "\n",
    "# Print the result\n",
    "if has_none_values:\n",
    "    print(f\"The DF has at least {none_count} rows with a None value.\")\n",
    "else:\n",
    "    print(\"The DF does not have any rows with None values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bookID=None, userID='ASS457AQPDIFZ', rating=5.0, timestamp=1409443200),\n",
       " Row(bookID=None, userID='A3NMH1KTLG7CWX', rating=5.0, timestamp=1398816000),\n",
       " Row(bookID=None, userID='A2LI5026JCXQBA', rating=4.0, timestamp=1398729600),\n",
       " Row(bookID=None, userID='AHNMXYVRDN1R9', rating=5.0, timestamp=1394323200),\n",
       " Row(bookID=None, userID='A2CAVTNQA2Y3IJ', rating=5.0, timestamp=1384560000),\n",
       " Row(bookID=None, userID='A2685NTFXLJJ1T', rating=5.0, timestamp=1377475200),\n",
       " Row(bookID=None, userID='A17TBLPM7H401J', rating=4.0, timestamp=1374364800),\n",
       " Row(bookID=None, userID='A1840OJGNFSBSN', rating=5.0, timestamp=900460800),\n",
       " Row(bookID=None, userID='A3ONKN7GMHG6K2', rating=5.0, timestamp=1400630400),\n",
       " Row(bookID=None, userID='A4LSI6PTX23BE', rating=5.0, timestamp=1400284800)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "none_df = books.filter(\n",
    "    (col('bookID').isNull()) |\n",
    "    (col('userID').isNull()) |\n",
    "    (col('rating').isNull()) |\n",
    "    (col('timestamp').isNull())\n",
    ")\n",
    "none_df.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows with missing values represent about 13% of the entire dataset.   \n",
    "Since there isn't a straightforward way of handling the missing values without affecting the results of the recommendations, the rows will simply be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44639628"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop missing values\n",
    "books = books.dropna()\n",
    "\n",
    "# Verify count\n",
    "books.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping rows with missing values, we still have around ~44.6M rows of data left to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Alphanumeric UserID to Int\n",
    "For use in the collaborative filtering algorithm, the `productID` and `userID` must be of type `int`.  \n",
    "\n",
    "To convert the alphanumeric UserIDs to unique integer IDs, we can first map each of the unique user IDs to a unique Integer ID, returning a new dictionary object in the k:v format --> `{'alpha_numID': 'new_intID'}`.  \n",
    "\n",
    "After this, we can parse the lines of data into `Rating` objects for use in Pyspark's collaborative filtering algorithm by using the dict obj to map the originial alphanumeric `userID` to the newly defined numeric IDs -- fulfilling the `int` dtype requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DF to RDD for Mapping UserIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1713353, 'A1C6M8LCIX4M6M', 5.0, 1123804800]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DF to RDD \n",
    "books_rdd = books.rdd.map(list)\n",
    "books_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A1C6M8LCIX4M6M'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all user IDs\n",
    "user_ids = books_rdd.map(lambda x: x[1])\n",
    "\n",
    "# Verify first line is user ID\n",
    "user_ids.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all unique userIDs for mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=22104Kb max_used=22106Kb free=108967Kb\n",
      " bounds [0x00000001091e0000, 0x000000010a7a0000, 0x00000001111e0000]\n",
      " total_blobs=8899 nmethods=7950 adapters=861\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14212140"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique user IDs\n",
    "user_ids = user_ids.distinct()\n",
    "\n",
    "# Verify users total 14M+\n",
    "user_ids.count() # resource intensive line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique users in the dataset is 14M+. Now each unique alphanumeric userID can be mapped to a new unique integer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# map user IDs to unique Int ID -> returns a dictionary\n",
    "user_ids_mapped = user_ids.zipWithUniqueId().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast to all nodes for efficient lookup\n",
    "broadcasted_dict = sc.broadcast(user_ids_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to Mllib Rating object\n",
    "ratings = books_rdd.map(lambda r: Row(userID=int(broadcasted_dict.value.get(r[1])), \n",
    "                                      bookID=int(r[0]), \n",
    "                                      rating=float(r[2]), \n",
    "                                      timestamp=r[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(userID=7, bookID=1713353, rating=5.0, timestamp=1123804800)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify correctly formatted Row object\n",
    "ratings.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has now been formatted appropriately for the collaborative filtering algorithm:  \n",
    "Rows of `Rating(int(user), int(product), float(rating), long(timestamp))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Back to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('userID', IntegerType(), False),\n",
    "    StructField('bookID', IntegerType(), False),\n",
    "    StructField('rating', FloatType(), False),\n",
    "    StructField('timestamp', LongType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to DF for more efficient processing\n",
    "ratings_df = spark.createDataFrame(ratings, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userID| bookID|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     7|1713353|   5.0|1123804800|\n",
      "|    15|1713353|   5.0|1112140800|\n",
      "|    12|1713353|   5.0|1081036800|\n",
      "|     3|1713353|   5.0|1077321600|\n",
      "|    31|1713353|   5.0|1475452800|\n",
      "|     5|1713353|   5.0|1469750400|\n",
      "|     8|1713353|   5.0|1466380800|\n",
      "|     0|1713353|   5.0|1461456000|\n",
      "|     6|1713353|   5.0|1455408000|\n",
      "|    14|1713353|   5.0|1453593600|\n",
      "+------+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userID', 'int'),\n",
       " ('bookID', 'int'),\n",
       " ('rating', 'float'),\n",
       " ('timestamp', 'bigint')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm dtypes\n",
    "ratings_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset Data for Testing with Various Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Fractions for 20M, 10M, 1M and 500K rows\n",
    "frac_20M, frac_10M, frac_1M, frac_500K, frac_10K = [0.4477, 0.2238, 0.0224, 0.0112, 0.0224]\n",
    "\n",
    "\n",
    "# Subset data \n",
    "books_subset_10K = ratings_df.sample(withReplacement=False, fraction=frac_10K)\n",
    "books_subset_500K = ratings_df.sample(withReplacement=False, fraction=frac_500K)\n",
    "books_subset_1M = ratings_df.sample(withReplacement=False, fraction=frac_1M)\n",
    "books_subset_10M = ratings_df.sample(withReplacement=False, fraction=frac_10M)\n",
    "books_subset_20M = ratings_df.sample(withReplacement=False, fraction=frac_20M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into Train/Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = books_subset_10K.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Recommendation Model using ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These params are chosen as a baseline based on benchmarking results [here.](http://mymedialite.net/examples/datasets.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparametes\n",
    "RANK = 10\n",
    "MAX_ITER = 15\n",
    "REG_PARAM = 0.05\n",
    "\n",
    "# Define columns\n",
    "COL_USER = \"userID\"\n",
    "COL_ITEM = \"bookID\"\n",
    "COL_RATING = \"rating\"\n",
    "COL_PREDICTION = \"prediction\"\n",
    "COL_TIMESTAMP = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of recommended books\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/19 11:02:48 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/11/19 11:02:48 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Build a baseline model with 10K rows only\n",
    "als = ALS(\n",
    "    maxIter=MAX_ITER,\n",
    "    rank=RANK,\n",
    "    regParam=REG_PARAM,\n",
    "    userCol=COL_USER,\n",
    "    itemCol=COL_ITEM,\n",
    "    ratingCol=COL_RATING,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('../models/baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model \n",
    "# loaded_model = ALS.load('../models/model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with model\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root mean squared error for the model is: 4.719012752097297\n"
     ]
    }
   ],
   "source": [
    "print(f\"The root mean squared error for the model is: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "# predicted_ratings = model.predictAll(test).map(lambda x: ((x[0], x[1]), x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\n",
    "#     \"RMSE score = {}\".format(evaluations.rmse()),\n",
    "#     \"MAE score = {}\".format(evaluations.mae()),\n",
    "#     \"R2 score = {}\".format(evaluations.rsquared()),\n",
    "#     \"Explained variance score = {}\".format(evaluations.exp_var()),\n",
    "#     sep=\"\\n\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build generic ALS Model\n",
    "als2 = ALS(userCol=COL_USER, \n",
    "          itemCol=COL_ITEM, \n",
    "          ratingCol=COL_RATING,\n",
    "          coldStartStrategy=\"drop\", \n",
    "          nonnegative=True, \n",
    "          implicitPrefs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparams for CV\n",
    "param_grid = ParamGridBuilder()\\\n",
    "             .addGrid(als2.rank, [5, 20, 40, 80])\\\n",
    "             .addGrid(als2.maxIter, [5, 100, 250, 300])\\\n",
    "             .addGrid(als2.regParam, [0.05, 0.1, 1.5])\\\n",
    "             .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", \n",
    "                                labelCol=\"rating\", \n",
    "                                predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Cross-Validation for best hyperparams\n",
    "cv = CrossValidator(estimator=als2,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5, \n",
    "                    parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cv on training data\n",
    "model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best combinations of values from CV\n",
    "# best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
