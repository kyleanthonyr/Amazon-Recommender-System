{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges This Week\n",
    "\n",
    "* With `Surprise` library, processing time too long on the large dataset\n",
    "* Tried `Pyspark`, to take advantage of it's distributed processing:\n",
    "    - mapping `str` type userIDs to `int` type userIDs was necessary\n",
    "    - processing time for training/evaluating extremely long due to ALS algo --> Tried using a smaller subset of data, still too long\n",
    "    - Terrible RMSE after evaluating baseline model (RMSE: 4.719, peviously with `Surprise`, it was 0.99-1.1)\n",
    "    - pyspark session abruptly stopping due to running out of memory (?) during cross-val, even with smaller subset of data\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "* Revert back to `Surprise` library for modeling since no need to map `str` type userIDs to `int` type, and more options for algorithms\n",
    "* Most likely will have to only use a fraction of the overall dataset instead of entire dataset due to processing time\n",
    "\n",
    "\n",
    "**Note**: Dataset contained 51M+ rows, and 46M+ rows after dropping rows containing `None` type values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, FloatType, StringType, LongType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from surprise import Reader, Dataset, SVD, NMF, accuracy\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from surprise import KNNBasic, KNNBaseline, KNNWithMeans\n",
    "from surprise.prediction_algorithms.slope_one import SlopeOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/10 17:14:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kyles-mbp.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Book Ratings</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x13068da90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[6]\").appName('Book Ratings')\\\n",
    "                            .config('spark.executor.memory', '8g')\\\n",
    "                            .config('spark.driver.memory', '4g')\\\n",
    "                            .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset into Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get underlying Spark Context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new schema\n",
    "schema = StructType()\\\n",
    "        .add('bookID', IntegerType(), nullable=False)\\\n",
    "        .add('userID', StringType(), nullable=False)\\\n",
    "        .add('rating', FloatType(), nullable=False)\\\n",
    "        .add('timestamp', LongType(), nullable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(bookID=1713353, userID='A1C6M8LCIX4M6M', rating=5.0, timestamp=1123804800)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data into PySpark DF\n",
    "books = spark.read.format('csv').schema(schema).load('../data/Books.csv')\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bookID: integer (nullable = true)\n",
      " |-- userID: string (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify data types\n",
    "books.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is returned in the format `(int(bookID), str(userID), float(rating), long(timestamp))`.\n",
    "\n",
    "To parse into a `PySpark` `Rating` object, it is expected to be in the format `(int(userID), int(bookID), float(rating), long(timestamp))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for `NoneType` Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DF has at least 6671993 rows with a None value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for None values in the DF\n",
    "none_count = books.filter(\n",
    "    (col(\"bookID\").isNull()) |\n",
    "    (col(\"userID\").isNull()) |\n",
    "    (col(\"rating\").isNull()) |\n",
    "    (col(\"timestamp\").isNull())\n",
    ").count()\n",
    "has_none_values = none_count > 0\n",
    "\n",
    "# Print the result\n",
    "if has_none_values:\n",
    "    print(f\"The DF has at least {none_count} rows with a None value.\")\n",
    "else:\n",
    "    print(\"The DF does not have any rows with None values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bookID=None, userID='ASS457AQPDIFZ', rating=5.0, timestamp=1409443200),\n",
       " Row(bookID=None, userID='A3NMH1KTLG7CWX', rating=5.0, timestamp=1398816000),\n",
       " Row(bookID=None, userID='A2LI5026JCXQBA', rating=4.0, timestamp=1398729600),\n",
       " Row(bookID=None, userID='AHNMXYVRDN1R9', rating=5.0, timestamp=1394323200),\n",
       " Row(bookID=None, userID='A2CAVTNQA2Y3IJ', rating=5.0, timestamp=1384560000),\n",
       " Row(bookID=None, userID='A2685NTFXLJJ1T', rating=5.0, timestamp=1377475200),\n",
       " Row(bookID=None, userID='A17TBLPM7H401J', rating=4.0, timestamp=1374364800),\n",
       " Row(bookID=None, userID='A1840OJGNFSBSN', rating=5.0, timestamp=900460800),\n",
       " Row(bookID=None, userID='A3ONKN7GMHG6K2', rating=5.0, timestamp=1400630400),\n",
       " Row(bookID=None, userID='A4LSI6PTX23BE', rating=5.0, timestamp=1400284800)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify what rows with None values look like\n",
    "none_df = books.filter(\n",
    "    (col('bookID').isNull()) |\n",
    "    (col('userID').isNull()) |\n",
    "    (col('rating').isNull()) |\n",
    "    (col('timestamp').isNull())\n",
    ")\n",
    "none_df.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows with missing values represent about 13% of the entire dataset.   \n",
    "Since there isn't a straightforward way of handling the missing values without affecting the results of the recommendations, the rows will simply be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "books = books.dropna()\n",
    "\n",
    "# Verify count\n",
    "# books.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping rows with missing values, we still have around ~44.6M rows of data left to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Dataset based on Books Titles\n",
    "Due to the large size of the dataset and processing issues, the dataset will be downsampled to about 1,000,000 rows based on books.\n",
    "\n",
    "**Rationale**:\n",
    "Since each book on average received about 17 distinct ratings, downsampling the dataset to about 1M rows will require about 58,000 unique books.  \n",
    "This is still a very large dataset and should be able to return decent results for our recommendation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DF to RDD for Mapping UserIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1713353, 'A1C6M8LCIX4M6M', 5.0, 1123804800]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DF to RDD \n",
    "books_rdd = books.rdd.map(list)\n",
    "books_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1713353"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all book IDs\n",
    "book_ids = books_rdd.map(lambda x: x[0])\n",
    "\n",
    "# Verify result\n",
    "book_ids.first()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all unique bookIDs for mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=20344Kb max_used=20357Kb free=110727Kb\n",
      " bounds [0x000000010a9e0000, 0x000000010bdf0000, 0x00000001129e0000]\n",
      " total_blobs=8576 nmethods=7628 adapters=861\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2266596 total books in the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get all unique book IDs\n",
    "book_ids = book_ids.distinct()\n",
    "\n",
    "# Count total book titles \n",
    "total_books = book_ids.count()\n",
    "\n",
    "print(f\"There are {total_books} total books in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Random Sample of 58,000 books from list of unique books IDs\n",
    "book_ids_58k = book_ids.sample(withReplacement=False, fraction=0.026, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ids_5k = book_ids.sample(withReplacement=False, fraction=0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book_ids_58k.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Broadcast values to nodes and perform collect transformation to be used in filter\n",
    "broadcasted_book_ids = sc.broadcast(set(book_ids_58k.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_5k = sc.broadcast(set(book_ids_5k.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_5k = books_rdd.filter(lambda x: x[0] in bb_5k.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user ids and ratings for subset of 58K books\n",
    "books_58k = books_rdd.filter(lambda x: x[0] in broadcasted_book_ids.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books_58k.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                (0 + 6) / 16]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kylerodriguez/miniconda3/envs/book-recommender/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kylerodriguez/miniconda3/envs/book-recommender/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kylerodriguez/miniconda3/envs/book-recommender/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kylerodriguez/Desktop/Springboard Data Science Track/amazon-recommender/notebooks/02-kar-preprocessing-and-modeling.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kylerodriguez/Desktop/Springboard%20Data%20Science%20Track/amazon-recommender/notebooks/02-kar-preprocessing-and-modeling.ipynb#Y116sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m books_5k\u001b[39m.\u001b[39;49mcount()\n",
      "File \u001b[0;32m~/miniconda3/envs/book-recommender/lib/python3.12/site-packages/pyspark/rdd.py:2316\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   2296\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
      "File \u001b[0;32m~/miniconda3/envs/book-recommender/lib/python3.12/site-packages/pyspark/rdd.py:2291\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[NumberOrArray]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNumberOrArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2271\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   2292\u001b[0m         \u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd\n\u001b[1;32m   2293\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/book-recommender/lib/python3.12/site-packages/pyspark/rdd.py:2044\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[1;32m   2041\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 2044\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m   2045\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/miniconda3/envs/book-recommender/lib/python3.12/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/miniconda3/envs/book-recommender/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/envs/book-recommender/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/miniconda3/envs/book-recommender/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/book-recommender/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "books_5k.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully filtering, the dataset has been reduced to just over 1M rows, and was downsampled based on the book IDs as opposed to the user IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert first to Spark DF\n",
    "books_58k_df = books_58k.toDF()\n",
    "\n",
    "# Then convert to Pandas DF from use with the Surprise library\n",
    "books_58k_df = books_58k_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "books_5k_df = books_5k.toDF()\n",
    "\n",
    "books_5k = books_5k_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_1</th>\n",
       "      <th>_2</th>\n",
       "      <th>_3</th>\n",
       "      <th>_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6173624</td>\n",
       "      <td>AIRJKGFFI1POJ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1519171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6173624</td>\n",
       "      <td>AOS0X376MV1TN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1519084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6173624</td>\n",
       "      <td>AXJQ4MBM9TY03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1518652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6173624</td>\n",
       "      <td>A2XI8CU2845UXM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1518048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6173624</td>\n",
       "      <td>A19FZP5PZXAOD3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1517616000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        _1              _2   _3          _4\n",
       "0  6173624   AIRJKGFFI1POJ  5.0  1519171200\n",
       "1  6173624   AOS0X376MV1TN  2.0  1519084800\n",
       "2  6173624   AXJQ4MBM9TY03  3.0  1518652800\n",
       "3  6173624  A2XI8CU2845UXM  5.0  1518048000\n",
       "4  6173624  A19FZP5PZXAOD3  5.0  1517616000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_5k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_1</th>\n",
       "      <th>_2</th>\n",
       "      <th>_3</th>\n",
       "      <th>_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008343</td>\n",
       "      <td>A1RB3KF8ZQ43DZ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1203811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008343</td>\n",
       "      <td>A20QI7NG8SFN05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1199750400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        _1              _2   _3          _4\n",
       "0  2008343  A1RB3KF8ZQ43DZ  4.0  1203811200\n",
       "1  2008343  A20QI7NG8SFN05  3.0  1199750400"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm data structure\n",
    "books_58k_df.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Recommender System with Python's Surprise Library\n",
    "\n",
    "The data is returned in the format `(bookID, userID, rating, timestamp)`. \n",
    "To parse into a `Surprise` dataframe, it is expected to be in the format `(userID, bookID, rating, timestamp)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide column headings to dataframe\n",
    "books = books_58k_df.set_axis(['bookID', 'userID', 'rating', 'timestamp'], axis=1)\n",
    "books_5k = books_5k.set_axis(['bookID', 'userID', 'rating', 'timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>bookID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIRJKGFFI1POJ</td>\n",
       "      <td>6173624</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1519171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS0X376MV1TN</td>\n",
       "      <td>6173624</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1519084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AXJQ4MBM9TY03</td>\n",
       "      <td>6173624</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1518652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2XI8CU2845UXM</td>\n",
       "      <td>6173624</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1518048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A19FZP5PZXAOD3</td>\n",
       "      <td>6173624</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1517616000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userID   bookID  rating   timestamp\n",
       "0   AIRJKGFFI1POJ  6173624     5.0  1519171200\n",
       "1   AOS0X376MV1TN  6173624     2.0  1519084800\n",
       "2   AXJQ4MBM9TY03  6173624     3.0  1518652800\n",
       "3  A2XI8CU2845UXM  6173624     5.0  1518048000\n",
       "4  A19FZP5PZXAOD3  6173624     5.0  1517616000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset just user ID, book ID and ratings (out of 5 stars)\n",
    "books = books[['userID', 'bookID', 'rating', 'timestamp']]\n",
    "books_5k = books_5k[['userID', 'bookID', 'rating', 'timestamp']]\n",
    "\n",
    "books_5k.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the pandas dataframe is in an acceptable format to be used by Suprise's algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data into a Surprise DF\n",
    "reader = Reader(rating_scale=(1.0, 5.0))\n",
    "data = Dataset.load_from_df(books[['userID', 'bookID', 'rating']], reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data into a Surprise DF\n",
    "reader2 = Reader(rating_scale=(1.0, 5.0))\n",
    "data2 = Dataset.load_from_df(books_5k[['userID', 'bookID', 'rating']], reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kylerodriguez/Desktop/Springboard Data Science Track/amazon-recommender/notebooks/02-kar-preprocessing-and-modeling.ipynb Cell 48\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kylerodriguez/Desktop/Springboard%20Data%20Science%20Track/amazon-recommender/notebooks/02-kar-preprocessing-and-modeling.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Perform train test split\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kylerodriguez/Desktop/Springboard%20Data%20Science%20Track/amazon-recommender/notebooks/02-kar-preprocessing-and-modeling.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainset, testset \u001b[39m=\u001b[39m train_test_split(data2, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform train test split\n",
    "trainset, testset = train_test_split(data2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9859147129145115"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define baseline SVD \n",
    "model = SVD()\n",
    "\n",
    "# Train and predict\n",
    "model.fit(trainset)\n",
    "preds = model.test(testset)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy.rmse(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the baseline SVD model give an RMSE value of 0.989, or roughly an error of 1 full star-rating on average, on a 1-5 star rating scale. While this is not a terrible score, it can be improved upon for more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Cross-validation to Randomize Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9880  0.9869  0.9911  0.9878  0.9887  0.9885  0.0014  \n",
      "MAE (testset)     0.7244  0.7232  0.7255  0.7239  0.7236  0.7241  0.0008  \n",
      "Fit time          10.12   10.36   10.75   10.54   13.68   11.09   1.31    \n",
      "Test time         1.21    1.06    1.17    1.10    0.72    1.05    0.17    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.98803048, 0.98694421, 0.99110749, 0.98777696, 0.98870302]),\n",
       " 'test_mae': array([0.72436212, 0.72322946, 0.72554698, 0.72386566, 0.72362888]),\n",
       " 'fit_time': (10.117323875427246,\n",
       "  10.36492109298706,\n",
       "  10.753936052322388,\n",
       "  10.540676832199097,\n",
       "  13.675766944885254),\n",
       " 'test_time': (1.2062747478485107,\n",
       "  1.0624489784240723,\n",
       "  1.166262149810791,\n",
       "  1.1043531894683838,\n",
       "  0.7231988906860352)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(model, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with a simple cross-validation to rule out randomness in train/test split, the average RMSE score was still ~0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform GridSearch CV to find Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9837273688883142\n",
      "{'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"n_epochs\": [5, 10, 15, 20], \n",
    "    \"lr_all\": [0.002, 0.005, 0.01, 0.1], \n",
    "    \"reg_all\": [0.1, 0.4, 0.6]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=5)\n",
    "\n",
    "# fit train data\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score[\"rmse\"])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9915724647475507\n",
      "{'n_epochs': 150, 'lr_all': 0.002, 'reg_all': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"n_epochs\": [20, 50, 150],\n",
    "    \"lr_all\": [0.002, 0.005, 0.01, 0.1],\n",
    "    \"reg_all\": [0.1, 0.4, 0.6]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=5)\n",
    "\n",
    "# fit train data\n",
    "gs.fit(data2)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score[\"rmse\"])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9837  0.9837  0.9848  0.9821  0.9846  0.9838  0.0010  \n",
      "MAE (testset)     0.7194  0.7205  0.7199  0.7200  0.7211  0.7202  0.0006  \n",
      "Fit time          13.37   11.99   11.00   10.57   12.30   11.85   0.99    \n",
      "Test time         1.29    0.75    0.76    0.76    0.77    0.87    0.21    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.98368286, 0.98374204, 0.98476931, 0.98206963, 0.98459273]),\n",
       " 'test_mae': array([0.71938808, 0.72050783, 0.71986622, 0.72001344, 0.72113289]),\n",
       " 'fit_time': (13.3734769821167,\n",
       "  11.986736059188843,\n",
       "  11.001637935638428,\n",
       "  10.574107885360718,\n",
       "  12.299041032791138),\n",
       " 'test_time': (1.2938790321350098,\n",
       "  0.7538900375366211,\n",
       "  0.7587981224060059,\n",
       "  0.7577779293060303,\n",
       "  0.7703983783721924)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run best model and perform cross-validation\n",
    "tuned_model = SVD(n_epochs=20, lr_all=0.01, reg_all=0.1)\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(tuned_model, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning using GridSearch, the score improved only slightly to ~0.98 on average, after cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Neighbors-based Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure sim options\n",
    "sim_options = {\n",
    "    'name': 'msd',\n",
    "    'min_support': 10,\n",
    "    'user_based': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0359539018955202"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_basic = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "knn_basic.fit(trainset=trainset)\n",
    "\n",
    "knn_basic_preds = knn_basic.test(testset)\n",
    "\n",
    "accuracy.rmse(knn_basic_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9857962734509415"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline KNNBasic model\n",
    "knn = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "# Fit and predict\n",
    "knn.fit(trainset=trainset)\n",
    "\n",
    "knn_preds = knn.test(testset=testset)\n",
    "\n",
    "# Evaluate model \n",
    "accuracy.rmse(knn_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.035044683405012"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_means = KNNWithMeans(sim_options=sim_options)\n",
    "\n",
    "knn_means.fit(trainset=trainset)\n",
    "\n",
    "means_preds = knn_means.test(testset)\n",
    "\n",
    "accuracy.rmse(means_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Negative Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0565001929351474"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = NMF()\n",
    "\n",
    "nmf.fit(trainset)\n",
    "\n",
    "nmf_preds = nmf.test(testset)\n",
    "\n",
    "accuracy.rmse(nmf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SlopeOne Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0486803955595436"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope_one = SlopeOne()\n",
    "\n",
    "slope_one.fit(trainset)\n",
    "\n",
    "slope_preds = slope_one.test(testset)\n",
    "\n",
    "accuracy.rmse(slope_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(evaluator, trainset=trainset, testset=testset):\n",
    "\n",
    "    # Get name of evaluator\n",
    "    name = evaluator.__name__\n",
    "\n",
    "    # Instantiate model\n",
    "    model = evaluator()\n",
    "\n",
    "    # Fit model to trainset\n",
    "    model.fit(trainset)\n",
    "\n",
    "    # Predict on test set\n",
    "    preds = model.test(testset)\n",
    "\n",
    "    # Return RMSE metric\n",
    "    rmse = accuracy.rmse(preds)\n",
    "\n",
    "    return {\"model\": name, \"RMSE\": rmse}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'SVD', 'RMSE': 1.0126084945329576}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_model(SVD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "predict_model(KNNBaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
