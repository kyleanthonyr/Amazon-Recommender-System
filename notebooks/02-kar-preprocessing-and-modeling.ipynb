{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges This Week\n",
    "\n",
    "* With `Surprise` library, processing time too long on the large dataset\n",
    "* Tried `Pyspark`, to take advantage of it's distributed processing:\n",
    "    - mapping `str` type userIDs to `int` type userIDs was necessary\n",
    "    - processing time for training/evaluating extremely long due to ALS algo --> Tried using a smaller subset of data, still too long\n",
    "    - Terrible RMSE after evaluating baseline model (RMSE: 4.719, peviously with `Surprise`, it was 0.99-1.1)\n",
    "    - pyspark session abruptly stopping due to running out of memory (?) during cross-val, even with smaller subset of data\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "* Revert back to `Surprise` library for modeling since no need to map `str` type userIDs to `int` type, and more options for algorithms\n",
    "* Most likely will have to only use a fraction of the overall dataset instead of entire dataset due to processing time\n",
    "\n",
    "\n",
    "**Note**: Dataset contained 51M+ rows, and 46M+ rows after dropping rows containing `None` type values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, FloatType, StringType, LongType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from surprise import Reader, Dataset, SVD, accuracy\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from surprise import KNNBasic, KNNBaseline, KNNWithMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/09 22:09:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kyles-mbp.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Book Ratings</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x120e77b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[6]\").appName('Book Ratings')\\\n",
    "                            .config('spark.executor.memory', '8g')\\\n",
    "                            .config('spark.driver.memory', '4g')\\\n",
    "                            .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset into Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get underlying Spark Context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new schema\n",
    "schema = StructType()\\\n",
    "        .add('bookID', IntegerType(), nullable=False)\\\n",
    "        .add('userID', StringType(), nullable=False)\\\n",
    "        .add('rating', FloatType(), nullable=False)\\\n",
    "        .add('timestamp', LongType(), nullable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(bookID=1713353, userID='A1C6M8LCIX4M6M', rating=5.0, timestamp=1123804800)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data into PySpark DF\n",
    "books = spark.read.format('csv').schema(schema).load('../data/Books.csv')\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bookID: integer (nullable = true)\n",
      " |-- userID: string (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify data types\n",
    "books.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is returned in the format `(int(bookID), str(userID), float(rating), long(timestamp))`.\n",
    "\n",
    "To parse into a `PySpark` `Rating` object, it is expected to be in the format `(int(userID), int(bookID), float(rating), long(timestamp))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for `NoneType` Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DF has at least 6671993 rows with a None value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for None values in the DF\n",
    "none_count = books.filter(\n",
    "    (col(\"bookID\").isNull()) |\n",
    "    (col(\"userID\").isNull()) |\n",
    "    (col(\"rating\").isNull()) |\n",
    "    (col(\"timestamp\").isNull())\n",
    ").count()\n",
    "has_none_values = none_count > 0\n",
    "\n",
    "# Print the result\n",
    "if has_none_values:\n",
    "    print(f\"The DF has at least {none_count} rows with a None value.\")\n",
    "else:\n",
    "    print(\"The DF does not have any rows with None values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bookID=None, userID='ASS457AQPDIFZ', rating=5.0, timestamp=1409443200),\n",
       " Row(bookID=None, userID='A3NMH1KTLG7CWX', rating=5.0, timestamp=1398816000),\n",
       " Row(bookID=None, userID='A2LI5026JCXQBA', rating=4.0, timestamp=1398729600),\n",
       " Row(bookID=None, userID='AHNMXYVRDN1R9', rating=5.0, timestamp=1394323200),\n",
       " Row(bookID=None, userID='A2CAVTNQA2Y3IJ', rating=5.0, timestamp=1384560000),\n",
       " Row(bookID=None, userID='A2685NTFXLJJ1T', rating=5.0, timestamp=1377475200),\n",
       " Row(bookID=None, userID='A17TBLPM7H401J', rating=4.0, timestamp=1374364800),\n",
       " Row(bookID=None, userID='A1840OJGNFSBSN', rating=5.0, timestamp=900460800),\n",
       " Row(bookID=None, userID='A3ONKN7GMHG6K2', rating=5.0, timestamp=1400630400),\n",
       " Row(bookID=None, userID='A4LSI6PTX23BE', rating=5.0, timestamp=1400284800)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify what rows with None values look like\n",
    "none_df = books.filter(\n",
    "    (col('bookID').isNull()) |\n",
    "    (col('userID').isNull()) |\n",
    "    (col('rating').isNull()) |\n",
    "    (col('timestamp').isNull())\n",
    ")\n",
    "none_df.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows with missing values represent about 13% of the entire dataset.   \n",
    "Since there isn't a straightforward way of handling the missing values without affecting the results of the recommendations, the rows will simply be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44639628"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop missing values\n",
    "books = books.dropna()\n",
    "\n",
    "# Verify count\n",
    "books.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping rows with missing values, we still have around ~44.6M rows of data left to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Dataset based on Books Titles\n",
    "Due to the large size of the dataset and processing issues, the dataset will be downsampled to about 1,000,000 rows based on books.\n",
    "\n",
    "**Rationale**:\n",
    "Since each book on average received about 17 distinct ratings, downsampling the dataset to about 1M rows will require about 58,000 unique books.  \n",
    "This is still a very large dataset and should be able to return decent results for our recommendation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DF to RDD for Mapping UserIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1713353, 'A1C6M8LCIX4M6M', 5.0, 1123804800]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DF to RDD \n",
    "books_rdd = books.rdd.map(list)\n",
    "books_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1713353"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all book IDs\n",
    "book_ids = books_rdd.map(lambda x: x[0])\n",
    "\n",
    "# Verify result\n",
    "book_ids.first()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all unique bookIDs for mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=22607Kb max_used=22617Kb free=108464Kb\n",
      " bounds [0x00000001089e0000, 0x000000010a020000, 0x00000001109e0000]\n",
      " total_blobs=9396 nmethods=8446 adapters=861\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2266596 total books in the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get all unique book IDs\n",
    "book_ids = book_ids.distinct()\n",
    "\n",
    "# Count total book titles \n",
    "total_books = book_ids.count()\n",
    "\n",
    "print(f\"There are {total_books} total books in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Random Sample of 58,000 books from list of unique books IDs\n",
    "book_ids_58k = book_ids.sample(withReplacement=False, fraction=0.026, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58851"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_ids_58k.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast values to nodes and perform collect transformation to be used in filter\n",
    "broadcasted_book_ids = sc.broadcast(set(book_ids_58k.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user ids and ratings for subset of 58K books\n",
    "books_58k = books_rdd.filter(lambda x: x[0] in broadcasted_book_ids.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1182213"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_58k.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully filtering, the dataset has been reduced to just over 1M rows, and was downsampled based on the book IDs as opposed to the user IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert first to Spark DF\n",
    "books_58k_df = books_58k.toDF()\n",
    "\n",
    "# Then convert to Pandas DF from use with the Surprise library\n",
    "books_58k_df = books_58k_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_1</th>\n",
       "      <th>_2</th>\n",
       "      <th>_3</th>\n",
       "      <th>_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008343</td>\n",
       "      <td>A1RB3KF8ZQ43DZ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1203811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008343</td>\n",
       "      <td>A20QI7NG8SFN05</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1199750400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        _1              _2   _3          _4\n",
       "0  2008343  A1RB3KF8ZQ43DZ  4.0  1203811200\n",
       "1  2008343  A20QI7NG8SFN05  3.0  1199750400"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm data structure\n",
    "books_58k_df.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Recommender System with Python's Surprise Library\n",
    "\n",
    "The data is returned in the format `(bookID, userID, rating, timestamp)`. \n",
    "To parse into a `Surprise` dataframe, it is expected to be in the format `(userID, bookID, rating, timestamp)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide column headings to dataframe\n",
    "books = books_58k_df.set_axis(['bookID', 'userID', 'rating', 'timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>bookID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1RB3KF8ZQ43DZ</td>\n",
       "      <td>2008343</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1203811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A20QI7NG8SFN05</td>\n",
       "      <td>2008343</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1199750400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3V9X42L3AI67I</td>\n",
       "      <td>1472933</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1360627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AG7E2YAPMFJVL</td>\n",
       "      <td>1472933</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1358640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARFK2WYVYT1QQ</td>\n",
       "      <td>1472933</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1347321600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userID   bookID  rating   timestamp\n",
       "0  A1RB3KF8ZQ43DZ  2008343     4.0  1203811200\n",
       "1  A20QI7NG8SFN05  2008343     3.0  1199750400\n",
       "2  A3V9X42L3AI67I  1472933     5.0  1360627200\n",
       "3   AG7E2YAPMFJVL  1472933     3.0  1358640000\n",
       "4   ARFK2WYVYT1QQ  1472933     5.0  1347321600"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset just user ID, book ID and ratings (out of 5 stars)\n",
    "books = books[['userID', 'bookID', 'rating', 'timestamp']]\n",
    "books.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the pandas dataframe is in an acceptable format to be used by Suprise's algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data into a Surprise DF\n",
    "reader = Reader(rating_scale=(1.0, 5.0))\n",
    "data = Dataset.load_from_df(books[['userID', 'bookID', 'rating']], reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train test split\n",
    "trainset, testset = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9854549930586427"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define baseline SVD \n",
    "model = SVD()\n",
    "\n",
    "# Train and predict\n",
    "model.fit(trainset)\n",
    "preds = model.test(testset)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy.rmse(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the baseline SVD model give an RMSE value of 0.989, or roughly an error of 1 full star-rating on average, on a 1-5 star rating scale. While this is not a terrible score, it can be improved upon for more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Cross-validation to Randomize Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9899  0.9868  0.9877  0.9876  0.9889  0.9882  0.0011  \n",
      "MAE (testset)     0.7257  0.7227  0.7234  0.7237  0.7242  0.7239  0.0010  \n",
      "Fit time          12.49   10.43   10.72   10.53   10.41   10.92   0.80    \n",
      "Test time         0.74    0.75    0.76    1.38    1.38    1.00    0.31    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.98988912, 0.98677177, 0.98768439, 0.9875813 , 0.98893956]),\n",
       " 'test_mae': array([0.72571196, 0.72268489, 0.72341569, 0.72367834, 0.72423129]),\n",
       " 'fit_time': (12.494518995285034,\n",
       "  10.432440996170044,\n",
       "  10.722224950790405,\n",
       "  10.526863813400269,\n",
       "  10.412353992462158),\n",
       " 'test_time': (0.7356030941009521,\n",
       "  0.7517130374908447,\n",
       "  0.7563676834106445,\n",
       "  1.3817241191864014,\n",
       "  1.3761780261993408)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(model, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with a simple cross-validation to rule out randomness in train/test split, the average RMSE score was still ~0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform GridSearch CV to find Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9837273688883142\n",
      "{'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"n_epochs\": [5, 10, 15, 20], \n",
    "    \"lr_all\": [0.002, 0.005, 0.01, 0.1], \n",
    "    \"reg_all\": [0.1, 0.4, 0.6]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=5)\n",
    "\n",
    "# fit train data\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score[\"rmse\"])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9837  0.9837  0.9848  0.9821  0.9846  0.9838  0.0010  \n",
      "MAE (testset)     0.7194  0.7205  0.7199  0.7200  0.7211  0.7202  0.0006  \n",
      "Fit time          13.37   11.99   11.00   10.57   12.30   11.85   0.99    \n",
      "Test time         1.29    0.75    0.76    0.76    0.77    0.87    0.21    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.98368286, 0.98374204, 0.98476931, 0.98206963, 0.98459273]),\n",
       " 'test_mae': array([0.71938808, 0.72050783, 0.71986622, 0.72001344, 0.72113289]),\n",
       " 'fit_time': (13.3734769821167,\n",
       "  11.986736059188843,\n",
       "  11.001637935638428,\n",
       "  10.574107885360718,\n",
       "  12.299041032791138),\n",
       " 'test_time': (1.2938790321350098,\n",
       "  0.7538900375366211,\n",
       "  0.7587981224060059,\n",
       "  0.7577779293060303,\n",
       "  0.7703983783721924)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run best model and perform cross-validation\n",
    "tuned_model = SVD(n_epochs=20, lr_all=0.01, reg_all=0.1)\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(tuned_model, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning using GridSearch, the score improved only slightly to ~0.98 on average, after cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Neighbors-based Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configure sim options\n",
    "# sim_options = {\n",
    "#     'name': 'msd',\n",
    "#     'min_support': 10,\n",
    "#     'user_based': False\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Baseline KNNBasic model\n",
    "# knn = KNNWithMeans(verbose=True, sim_options=sim_options)\n",
    "\n",
    "# # Fit and predict\n",
    "# knn.fit(trainset=trainset)\n",
    "\n",
    "# knn_preds = knn.test(testset=testset)\n",
    "\n",
    "# # Evaluate model \n",
    "# accuracy.rmse(knn_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
